{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9장 텐서플로 시작하기\n",
    "1. 설치\n",
    "2. 첫 번째 계산 그래프를 만들어서 세션에서 실행하기\n",
    "3. 계산 그래프 관리\n",
    "4. 노드 값의 생애주기\n",
    "5. 텐서플로를 이용한 선형 회귀\n",
    "6. 경사 하강법 구현\n",
    "7. 훈련 알고리즘에 데이터 주입\n",
    "8. 모델 저장과 복원\n",
    "9. 텐서보드로 그래프와 학습 곡선 시각화하기\n",
    "10. 이름 범위\n",
    "11. 모듈화\n",
    "12. 변수 공유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 텐서플로를 이용한 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape # the number of rows, columns\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] #c_: 세로 붙이기\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "# theta = (X^T * X)^-1 * X^T * y\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 경사 하강법 구현\n",
    "__Cautiojn: 경사 하강법을 사용할 때는 입력 특성 벡터를 정규화하는 것이 중요! 그렇지 않으면 훈련 속도가 매우 느려집니다.__\n",
    "1. 직접 그래디언트 계산\n",
    "2. 자동 미분 사용\n",
    "3. 옵티마이저 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # 정규화\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing_data_plus_bias)\n",
    "scaled_housing_data_plus_bias = scaler.transform(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.785181045532227\n",
      "Epoch 100 MSE = 5.156213760375977\n",
      "Epoch 200 MSE = 5.013348579406738\n",
      "Epoch 300 MSE = 4.956762313842773\n",
      "Epoch 400 MSE = 4.916631698608398\n",
      "Epoch 500 MSE = 4.887288570404053\n",
      "Epoch 600 MSE = 4.865750312805176\n",
      "Epoch 700 MSE = 4.849900722503662\n",
      "Epoch 800 MSE = 4.838207244873047\n",
      "Epoch 900 MSE = 4.829555034637451\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")  # reshape를 통해 1D to 2D shpee로\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0,), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "error = y_pred -y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "## 1. 직접 그래디언트 계산 --------------------------------------------\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "## 2. 자동 미분 사용 --------------------------------------------------\n",
    "#gradients = tf.gradients(mse, [theta])[0]\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "## 3. Optimizer -------------------------------------------------------\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "## --------------------------------------------------------------------\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch} MSE = {mse.eval()}')\n",
    "        sess.run(training_op)\n",
    "                           \n",
    "    best_theta = theta.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
